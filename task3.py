# -*- coding: utf-8 -*-
"""TASK3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xnLCke-ndBTsI6gsFwwYFjHD5GultrUq

Task 3

Objective: Develop a machine learning model to
classify restaurants based on their cuisines.

Steps:

Preprocess the dataset by handling missing values
and encoding categorical variables.
Split the data into training and testing sets.
Select a classification algorithm (e.g., logistic
regression, random forest) and train it on the
training data.
Evaluate the model's performance using
appropriate classification metrics (e.g., accuracy,
precision, recall) on the testing data.
Analyze the model's performance across different
cuisines and identify any challenges or biases.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential #type ignore
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#LETS LOAD THE DATASET AND DO EDA

df = pd.read_csv('/content/Dataset .csv')
print(df.head())

print(df.isnull().sum())
for col in df.select_dtypes(include=['object']).columns:
    df[col].fillna(df[col].mode()[0], inplace=True)

for col in df.select_dtypes(include=['number']).columns:
    df[col].fillna(df[col].median(), inplace=True)

#ENCODE THE CATEGORICAL COLUMNS
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

#CLASSIFY CUISINES AS TARGET VARIABLE
X = df.drop(columns=['Cuisines'])
y = df['Cuisines']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

class_counts = y.value_counts()
rare_classes = class_counts[class_counts < 2].index

df = df[~df['Cuisines'].isin(rare_classes)]

X = df.drop(columns=['Cuisines'])
y = df['Cuisines']

print(class_counts)

# Filter classes with at least 5 samples
min_samples = 5
class_counts = y.value_counts()
valid_classes = class_counts[class_counts >= min_samples].index
df = df[df['Cuisines'].isin(valid_classes)]

X = df.drop(columns=['Cuisines'])
y = df['Cuisines']

y.shape

X.shape

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

!pip install --upgrade xgboost scikit-learn

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

print(y_train)

print(y)

le_cuisine = LabelEncoder()
df["Cuisines"] = le_cuisine.fit_transform(df["Cuisines"])

print("Unique classes in y_train:", np.unique(y_train))

le_cuisine = LabelEncoder()
y = le_cuisine.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print("Unique classes in y_train:", np.unique(y_train))
#THE CLASSES ARE NOT ENCODED PROPERLY

import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report

for col in df.select_dtypes(include=['object']).columns:
    df.loc[:, col] = df[col].fillna(df[col].mode()[0])

for col in df.select_dtypes(include=['number']).columns:
    df.loc[:, col] = df[col].fillna(df[col].median())

label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df.loc[:, col] = le.fit_transform(df[col])
    label_encoders[col] = le

min_samples = 10
class_counts = df["Cuisines"].value_counts()
valid_classes = class_counts[class_counts >= min_samples].index
df = df[df["Cuisines"].isin(valid_classes)]

le_cuisine = LabelEncoder()
df.loc[:, "Cuisines"] = le_cuisine.fit_transform(df["Cuisines"])

=X = df.drop(columns=["Cuisines"])
y = df["Cuisines"]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print("Unique classes in y_train:", np.unique(y_train))
xgb_model = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(np.unique(y_train)),  # Ensure correct number of classes
    n_estimators=300,   # More trees for better learning
    max_depth=7,        # Deeper trees for better learning
    learning_rate=0.1,  # Standard learning rate
    subsample=0.9,      # Helps with generalization
    colsample_bytree=0.9,
    min_child_weight=3, # Reduce overfitting
    eval_metric="logloss"
)

xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
print("\nXGBoost Model Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report (XGBoost):\n", classification_report(y_test, y_pred_xgb))

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report


# Handle missing values correctly
for col in df.select_dtypes(include=['object']).columns:
    df.loc[:, col] = df[col].fillna(df[col].mode()[0])  # Fix FutureWarning

for col in df.select_dtypes(include=['number']).columns:
    df.loc[:, col] = df[col].fillna(df[col].median())  # Fix FutureWarning

# Encode categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df.loc[:, col] = le.fit_transform(df[col])
    label_encoders[col] = le

# **Filter out rare cuisines with < 25 samples**
min_samples = 25
class_counts = df["Cuisines"].value_counts()
valid_classes = class_counts[class_counts >= min_samples].index
df = df[df["Cuisines"].isin(valid_classes)]

# Encode target variable properly
le_cuisine = LabelEncoder()
df.loc[:, "Cuisines"] = le_cuisine.fit_transform(df["Cuisines"])

# Define features and target
X = df.drop(columns=["Cuisines"])
y = df["Cuisines"]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split (Stratify ensures balanced distribution)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Print unique classes in y_train
print("Unique classes in y_train:", np.unique(y_train))

# **Initialize XGBoost for multiclass classification**
xgb_model = xgb.XGBClassifier(
    objective="multi:softmax",  # Use soft probability predictions instead of strict classification
    num_class=len(np.unique(y_train)),  # Ensure correct number of classes
    n_estimators=300,   # More trees for better learning
    max_depth=5,        # Deeper trees for better learning
    learning_rate=0.01,  # Standard learning rate
    subsample=0.9,      # Helps with generalization
    colsample_bytree=0.9,
    min_child_weight=3, # Reduce overfitting
    eval_metric="logloss"
)

# Train the model
xgb_model.fit(X_train, y_train)

# Predictions (Use np.argmax to convert soft probabilities to class labels)
y_pred_xgb = xgb_model.predict(X_test)  # FIXED: Directly predict class labels

# Evaluate the Model
print("\nXGBoost Model Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report (XGBoost):\n", classification_report(y_test, y_pred_xgb, zero_division=1))

#VERY POOR ACCURACIES DUE TO MULTIPLE CLASSES WITH INCONSISTENT SAMPLE SIZE